{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Medium.com Articles to Markdown Files \n",
    "list of URLs -> markdown files in GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import html2text\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def get_html_element(element,soup) -> str:\n",
    "    \"\"\"\n",
    "    Searches for the first occurrence of a specified HTML element in a BeautifulSoup object and returns its text.\n",
    "\n",
    "    Parameters:\n",
    "    - element (str): The tag name of the HTML element to search for (e.g., 'h1', 'div').\n",
    "    - soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text of the first occurrence of the specified element if found; otherwise, an empty string.\n",
    "    \"\"\"\n",
    "    result = soup.find(element)\n",
    "    if result:\n",
    "        return result.text\n",
    "    else:\n",
    "        print(f\"No element ${element} found.\")\n",
    "        return \"\"\n",
    "\n",
    "def cut_text_at_marker(marker:str,text:str,beginning:bool):\n",
    "    \"\"\"\n",
    "    Cuts the text at the specified marker and returns the resulting substring. The function can return the\n",
    "    text after the first occurrence of the marker (if beginning is True) or before the last occurrence\n",
    "    of the marker (if beginning is False).\n",
    "    \"\"\"\n",
    "    # Find the index of the substring\n",
    "    cut_off_index = 0\n",
    "    if beginning:\n",
    "        cut_off_index = text.find(marker)\n",
    "    else:\n",
    "        cut_off_index = text.rfind(marker)\n",
    "    # Slice the string if the substring is found\n",
    "    newText = \"\"\n",
    "    if cut_off_index != -1:\n",
    "        if beginning:\n",
    "            newText = text[cut_off_index + len(marker):]\n",
    "        else:\n",
    "            newText = text[:cut_off_index]\n",
    "    return newText\n",
    "\n",
    "def process_url(url: str):\n",
    "    \"\"\"\n",
    "    Processes a single URL to extract content, convert to Markdown, and upload to GCS.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "        title = get_html_element('h1', soup)\n",
    "        title_name = title.lower().replace(\" \", \"-\").replace(\":\", \"\").replace(\".\", \"\").replace(\"/\", \"\").replace(\")\", \"\").replace(\"(\", \"\")\n",
    "\n",
    "        if not title:\n",
    "            print(f\"No title found for {url}\")\n",
    "            return\n",
    "\n",
    "        subtitle = get_html_element('h2', soup)\n",
    "\n",
    "        if not subtitle:\n",
    "            print(f\"No subtitle found for {url}\")\n",
    "            return\n",
    "\n",
    "        ### code blocks\n",
    "        html_content = html_content.replace(\"<pre\", \"```<pre\")\n",
    "        html_content = html_content.replace(\"</pre>\", \"</pre>```\")\n",
    "\n",
    "        ### text separators\n",
    "        # Find all elements with role=\"separator\"\n",
    "        separator_elements = soup.find_all(attrs={\"role\": \"separator\"})\n",
    "\n",
    "        # replace with <hr> element, markdown recognizes this\n",
    "        for element in separator_elements:\n",
    "            html_content = html_content.replace(str(element), \"<hr>\")\n",
    "\n",
    "        ### convert to markdown\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False  # preserve hyperlinks\n",
    "        markdown_text = converter.handle(html_content)\n",
    "\n",
    "        ### cut end\n",
    "        markdown_text = cut_text_at_marker('\\--', markdown_text, False)\n",
    "\n",
    "        ### cut beginning\n",
    "        markdown_text = cut_text_at_marker('Share', markdown_text, True)\n",
    "\n",
    "        ### get tags\n",
    "        pattern = r\"\\[\\s*([^\\]]+?)\\s*\\]\"\n",
    "        matches = re.findall(pattern, markdown_text)\n",
    "        tags = matches[-5:]\n",
    "\n",
    "        ### cut end part II: remove the tags from the content\n",
    "        pattern = r'\\[\\s*{}'.format(re.escape(tags[0]))\n",
    "        all_patterns = list(re.finditer(pattern, markdown_text))\n",
    "        first_tag = all_patterns[-1]\n",
    "        second_cutoff = first_tag.start()\n",
    "        if second_cutoff != -1:\n",
    "            markdown_text = markdown_text[:second_cutoff]\n",
    "\n",
    "        ### code blocks part II: remove empty lines\n",
    "        pattern = r'(^```$)(\\s*\\n\\s*)+'\n",
    "        # Replace matches with just the \"```\" line\n",
    "        markdown_text = re.sub(pattern, r'\\1\\n', markdown_text, flags=re.MULTILINE)\n",
    "\n",
    "        ### get formatted date\n",
    "        today = datetime.now()\n",
    "        formatted_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        markdown_dir = \"/Volumes/DataLakeActive/GCP-generative-ai/1-parsedURLs_markdown\"\n",
    "        os.makedirs(markdown_dir, exist_ok=True)\n",
    "\n",
    "        filename = f\"{formatted_date_str}-{title_name}.md\"\n",
    "        file_path = os.path.join(markdown_dir, filename)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(markdown_text)\n",
    "\n",
    "        # DESTINATION GCS Bucket\n",
    "        bucket_name = 'articles_extracted_markdown'\n",
    "        upload_to_gcs(file_path, filename, bucket_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        # Handle the error by prefixing the filename and uploading\n",
    "        error_filename = f\"ERROR_{formatted_date_str}-{title_name}.md\"\n",
    "        with open(error_filename, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Error processing URL: {url}\\n\\n{str(e)}\")\n",
    "        upload_to_gcs(error_filename, 'articles_extracted_markdown')\n",
    "\n",
    "def upload_to_gcs(file_path: str, filename: str, bucket_name: str):\n",
    "    \"\"\"Uploads a file to a GCS bucket.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    print(f\"Uploaded {filename} to gs://{bucket_name}/{filename}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print(\"Usage: python script.py <urls_file.txt>\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#    urls_file = sys.argv[1]\n",
    "\n",
    "\n",
    "\n",
    "# Specify the file path directly\n",
    "urls_file = \"/Volumes/DataLakeActive/GCP-generative-ai/0-jc_notebooks/temp_list.txt\"\n",
    "\n",
    "with open(urls_file, 'r') as f:\n",
    "    urls = [line.strip() for line in f]\n",
    "\n",
    "for url in urls:\n",
    "    process_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCP Documents\n",
    "\n",
    "DOES NOT HAVE ANY CONTENT IN FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 2024-09-01-google-models.md to gs://articles_extracted_markdown/2024-09-01-google-models.md\n",
      "Uploaded 2024-09-01-embeddings-apis-overview.md to gs://articles_extracted_markdown/2024-09-01-embeddings-apis-overview.md\n",
      "Uploaded 2024-09-01-document-understanding.md to gs://articles_extracted_markdown/2024-09-01-document-understanding.md\n",
      "Uploaded 2024-09-01-model-tuning-for-gemini-text-models.md to gs://articles_extracted_markdown/2024-09-01-model-tuning-for-gemini-text-models.md\n",
      "Uploaded 2024-09-01-tune-text-embeddings.md to gs://articles_extracted_markdown/2024-09-01-tune-text-embeddings.md\n",
      "Uploaded 2024-09-01-llamaindex-on-vertex-ai-for-rag-overview.md to gs://articles_extracted_markdown/2024-09-01-llamaindex-on-vertex-ai-for-rag-overview.md\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import html2text\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import re\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "def get_html_element(element,soup) -> str:\n",
    "    \"\"\"\n",
    "    Searches for the first occurrence of a specified HTML element in a BeautifulSoup object and returns its text.\n",
    "\n",
    "    Parameters:\n",
    "    - element (str): The tag name of the HTML element to search for (e.g., 'h1', 'div').\n",
    "    - soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML document.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text of the first occurrence of the specified element if found; otherwise, an empty string.\n",
    "    \"\"\"\n",
    "    result = soup.find(element)\n",
    "    if result:\n",
    "        return result.text\n",
    "    else:\n",
    "        print(f\"No element ${element} found.\")\n",
    "        return \"\"\n",
    "\n",
    "def cut_text_at_marker(marker:str,text:str,beginning:bool):\n",
    "    \"\"\"\n",
    "    Cuts the text at the specified marker and returns the resulting substring. The function can return the\n",
    "    text after the first occurrence of the marker (if beginning is True) or before the last occurrence\n",
    "    of the marker (if beginning is False).\n",
    "    \"\"\"\n",
    "    # Find the index of the substring\n",
    "    cut_off_index = 0\n",
    "    if beginning:\n",
    "        cut_off_index = text.find(marker)\n",
    "    else:\n",
    "        cut_off_index = text.rfind(marker)\n",
    "    # Slice the string if the substring is found\n",
    "    newText = \"\"\n",
    "    if cut_off_index != -1:\n",
    "        if beginning:\n",
    "            newText = text[cut_off_index + len(marker):]\n",
    "        else:\n",
    "            newText = text[:cut_off_index]\n",
    "    return newText\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Sanitizes a filename to be compatible with GCS object naming rules.\"\"\"\n",
    "    # Remove newline characters\n",
    "    filename = filename.replace('\\n', '')\n",
    "    # Replace disallowed characters with underscores\n",
    "    filename = re.sub(r'[^\\w\\s.-]', '_', filename)\n",
    "    # Replace multiple consecutive hyphens with a single hyphen\n",
    "    filename = re.sub(r'-+', '-', filename)\n",
    "    # Remove leading or trailing hyphens\n",
    "    filename = filename.strip('-')\n",
    "    return filename\n",
    "\n",
    "def process_url(url: str):\n",
    "    \"\"\"\n",
    "    Processes a single URL to extract content, convert to Markdown, \n",
    "    and upload to GCS.\n",
    "    \"\"\"\n",
    "    # Get the formatted date string outside the try block\n",
    "    formatted_date_str = datetime.now().strftime(\"%Y-%m-%d\") \n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        html_content = response.text\n",
    "\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "        title = get_html_element('h1', soup)\n",
    "        title_name = title.lower().replace(\" \", \"-\").replace(\":\", \"\").replace(\".\", \"\").replace(\"/\", \"\")\n",
    "\n",
    "        if not title:\n",
    "            print(f\"No title found for {url}\")\n",
    "            return\n",
    "\n",
    "        subtitle = get_html_element('h2', soup)\n",
    "\n",
    "        if not subtitle:\n",
    "            print(f\"No subtitle found for {url}\")\n",
    "            return\n",
    "\n",
    "        ### code blocks\n",
    "        html_content = html_content.replace(\"<pre\", \"```<pre\")\n",
    "        html_content = html_content.replace(\"</pre>\", \"</pre>```\")\n",
    "\n",
    "        ### text separators\n",
    "        # Find all elements with role=\"separator\"\n",
    "        separator_elements = soup.find_all(attrs={\"role\": \"separator\"})\n",
    "\n",
    "        # replace with <hr> element, markdown recognizes this\n",
    "        for element in separator_elements:\n",
    "            html_content = html_content.replace(str(element), \"<hr>\")\n",
    "\n",
    "        ### convert to markdown\n",
    "        converter = html2text.HTML2Text()\n",
    "        converter.ignore_links = False  # preserve hyperlinks\n",
    "        markdown_text = converter.handle(html_content)\n",
    "\n",
    "        ### cut end\n",
    "        markdown_text = cut_text_at_marker('\\--', markdown_text, False)\n",
    "\n",
    "        ### cut beginning\n",
    "        markdown_text = cut_text_at_marker('Share', markdown_text, True)\n",
    "\n",
    "        ### get tags\n",
    "        pattern = r\"\\[\\s*([^\\]]+?)\\s*\\]\"\n",
    "        matches = re.findall(pattern, markdown_text)\n",
    "        tags = matches[-5:]\n",
    "\n",
    "        ### cut end part II: remove the tags from the content\n",
    "        if tags:  # Check if the tags list is not empty\n",
    "            pattern = r'\\[\\s*{}'.format(re.escape(tags[0]))\n",
    "            all_patterns = list(re.finditer(pattern, markdown_text))\n",
    "            if all_patterns:\n",
    "                first_tag = all_patterns[-1]\n",
    "                second_cutoff = first_tag.start()\n",
    "                if second_cutoff != -1:\n",
    "                    markdown_text = markdown_text[:second_cutoff]\n",
    "\n",
    "        ### code blocks part II: remove empty lines\n",
    "        pattern = r'(^```$)(\\s*\\n\\s*)+'\n",
    "        # Replace matches with just the \"```\" line\n",
    "        markdown_text = re.sub(pattern, r'\\1\\n', markdown_text, flags=re.MULTILINE)\n",
    "\n",
    "        ### get formatted date\n",
    "        today = datetime.now()\n",
    "        formatted_date_str = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        markdown_dir = \"/Volumes/DataLakeActive/GCP-generative-ai/1-parsedURLs_markdown\"\n",
    "        os.makedirs(markdown_dir, exist_ok=True)\n",
    "\n",
    "        filename = f\"{formatted_date_str}-{sanitize_filename(title_name)}.md\"\n",
    "        file_path = os.path.join(markdown_dir, filename)\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(markdown_text)\n",
    "\n",
    "        # DESTINATION GCS Bucket\n",
    "        bucket_name = 'articles_extracted_markdown'\n",
    "        upload_to_gcs(file_path, filename, bucket_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "        error_filename = f\"ERROR_{formatted_date_str}-{title_name}.md\"\n",
    "        error_file_path = os.path.join(markdown_dir, error_filename)\n",
    "        with open(error_file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Error processing URL: {url}\\n\\n{str(e)}\")\n",
    "        upload_to_gcs(error_file_path, error_filename, bucket_name)\n",
    "\n",
    "def upload_to_gcs(file_path: str, filename: str, bucket_name: str):\n",
    "    \"\"\"Uploads a file to a GCS bucket.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    print(f\"Uploaded {filename} to gs://{bucket_name}/{filename}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print(\"Usage: python script.py <urls_file.txt>\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#    urls_file = sys.argv[1]\n",
    "\n",
    "\n",
    "\n",
    "# Specify the file path directly\n",
    "urls_file = \"/Volumes/DataLakeActive/GCP-generative-ai/0-jc_notebooks/temp_list.txt\"\n",
    "\n",
    "with open(urls_file, 'r') as f:\n",
    "    urls = [line.strip() for line in f]\n",
    "\n",
    "for url in urls:\n",
    "    process_url(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
